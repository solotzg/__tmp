set execution.checkpointing.interval = 2sec;
set state.backend = rocksdb;
set state.checkpoints.dir = ${s3_dir}/flink-state/checkpoint;
set state.savepoints.dir = ${s3_dir}/flink-state/savepoints;
set state.backend.incremental = true;

create database demo_flink;
create table demo_flink.t3_file
(
  a int PRIMARY KEY, b int
)
with (
  'connector'='filesystem',
  'path' = 'file://${csv_file_path}',
  'format'='csv'
);
create database demo_hudi;
CREATE TABLE demo_hudi.t3(
  a INT PRIMARY KEY NOT ENFORCED,
  b INT
) WITH (
  'connector' = 'hudi',
  'path' = '${s3_dir}/hudi',
  'table.type' = 'MERGE_ON_READ'
);
create table demo_flink.t3(a int PRIMARY KEY, b int) with (
  'connector'='kafka',
  'topic'='${kafka_topic}_base', 
  'properties.bootstrap.servers'='${kafka_address}',
  'properties.group.id'='pingcap-demo-group',
  'format'='canal-json',
  'scan.startup.mode'='earliest-offset'
);
create table demo_flink.t3_inc(a int PRIMARY KEY, b int) with (
  'connector'='kafka', 
  'topic'='${kafka_topic}', 
  'properties.bootstrap.servers'='${kafka_address}', 
  'properties.group.id'='pingcap-demo-group', 
  'format'='canal-json', 
  'scan.startup.mode'='earliest-offset'
);

INSERT INTO demo_hudi.t3 (select * from demo_flink.t3);

SET table.dml-sync=true;

INSERT INTO demo_flink.t3 (select * from demo_flink.t3_file);

RESET table.dml-sync;

INSERT INTO demo_flink.t3 (select * from demo_flink.t3_inc);

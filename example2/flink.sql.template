set execution.checkpointing.interval = 2sec;

create database demo_flink;
create table demo_flink.t2(a int PRIMARY KEY, b TIMESTAMP(3), c STRING) with('connector'='kafka', 'topic'='${kafka_topic}', 'properties.bootstrap.servers'='${kafka_address}', 'properties.group.id'='pingcap-demo-group', 'format'='canal-json', 'scan.startup.mode'='latest-offset');
create database demo_hudi;
CREATE TABLE demo_hudi.t2(
  a INT PRIMARY KEY NOT ENFORCED,
  b TIMESTAMP(3),
  `partition` VARCHAR(20)
)
PARTITIONED BY (`partition`)
WITH (
  'connector' = 'hudi',
  'path' = 'hdfs://${hdfs_address}/pingcap/demo/hudi_t2',
  'table.type' = 'MERGE_ON_READ',
  'read.streaming.enabled' = 'true',
  'read.streaming.check-interval' = '1'
);
INSERT INTO demo_hudi.t2(a,b,`partition`) (select a, b, DATE_FORMAT(b, 'yyyyMMdd') as `partition` from demo_flink.t2 where c is not null);

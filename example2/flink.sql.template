set execution.checkpointing.interval = 2sec;
set state.backend = rocksdb;
set state.checkpoints.dir = ${hdfs_address}/checkpoints;
set state.savepoints.dir = ${hdfs_address}/savepoints;
set state.backend.incremental = true;

create database demo_flink;
create table demo_flink.t2(a int PRIMARY KEY, b TIMESTAMP(3), c STRING) with('connector'='kafka', 'topic'='${kafka_topic}', 'properties.bootstrap.servers'='${kafka_address}', 'properties.group.id'='pingcap-demo-group', 'format'='canal-json', 'scan.startup.mode'='earliest-offset');
create database demo_hudi;
CREATE TABLE demo_hudi.t2(
  a INT PRIMARY KEY NOT ENFORCED,
  b TIMESTAMP(3),
  `partition` VARCHAR(20)
)
PARTITIONED BY (`partition`)
WITH (
  'connector' = 'hudi',
  'path' = '${hdfs_address}',
  'table.type' = 'MERGE_ON_READ'
);
INSERT INTO demo_hudi.t2(a,b,`partition`) (select a, b, DATE_FORMAT(b, 'yyyyMMdd') as `partition` from demo_flink.t2 where c is not null);

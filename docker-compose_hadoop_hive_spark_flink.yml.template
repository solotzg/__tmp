version: "3.3"
name: "${LAKE_HOUSE_COMPOSE_PROJECT_NAME}"
services:

  namenode:
    image: apachehudi/hudi-hadoop_2.8.4-namenode:latest
    hostname: namenode
    environment:
      - CLUSTER_NAME=hudi_hadoop284_hive232_spark244
    ports:
      - "${hadoop_web_port}:50070"
      - "${hdfs_port}:8020"
      # JVM debugging port (will be mapped to a random port on host)
      # - "5005"
    env_file:
      - ${HUDI_WS}/docker/compose/hadoop.env
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://namenode:50070" ]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - ${pingcap_demo_path}/.tmp.demo/third_party/hadoop/dfs/name:/hadoop/dfs/name

  datanode1:
    image: apachehudi/hudi-hadoop_2.8.4-datanode:latest
    environment:
      - CLUSTER_NAME=hudi_hadoop284_hive232_spark244
    env_file:
      - ${HUDI_WS}/docker/compose/hadoop.env
    ports:
      # - "50075:50075"
      - "${hadoop_datanode_port}:50010"
      # JVM debugging port (will be mapped to a random port on host)
      # - "5005"
    links:
      - "namenode"
      - "historyserver"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://datanode1:50075" ]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - namenode
    volumes:
      - ${pingcap_demo_path}/.tmp.demo/third_party/hadoop/dfs/data:/hadoop/dfs/data

  historyserver:
    image: apachehudi/hudi-hadoop_2.8.4-history:latest
    hostname: historyserver
    environment:
      - CLUSTER_NAME=hudi_hadoop284_hive232_spark244
    depends_on:
      - "namenode"
    links:
      - "namenode"
    ports:
      - "$historyserver_port:8188"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://historyserver:8188" ]
      interval: 30s
      timeout: 10s
      retries: 3
    env_file:
      - ${HUDI_WS}/docker/compose/hadoop.env
    volumes:
      - ${pingcap_demo_path}/.tmp.demo/third_party/historyserver:/hadoop/yarn/timeline

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    volumes:
      - ${pingcap_demo_path}/.tmp.demo/third_party/hive-metastore-postgresql:/var/lib/postgresql/data
    hostname: hive-metastore-postgresql

  hivemetastore:
    image: apachehudi/hudi-hadoop_2.8.4-hive_2.3.3:latest
    hostname: hivemetastore
    links:
      - "hive-metastore-postgresql"
      - "namenode"
    env_file:
      - ${HUDI_WS}/docker/compose/hadoop.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:50070 hive-metastore-postgresql:5432"
    # ports:
      # - "9083:9083"
      # JVM debugging port (will be mapped to a random port on host)
      # - "5005"
    healthcheck:
      test: [ "CMD", "nc", "-z", "hivemetastore", "9083" ]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - "hive-metastore-postgresql"
      - "namenode"

  hiveserver:
    image: apachehudi/hudi-hadoop_2.8.4-hive_2.3.3:latest
    hostname: hiveserver
    env_file:
      - ${HUDI_WS}/docker/compose/hadoop.env
    environment:
      SERVICE_PRECONDITION: "hivemetastore:9083"
    #ports:
      # - "$hiveserver_port:10000"
      # JVM debugging port (will be mapped to a random port on host)
      # - "5005"
    depends_on:
      - "hivemetastore"
    links:
      - "hivemetastore"
      - "hive-metastore-postgresql"
      - "namenode"
    volumes:
      - ${HUDI_WS}:/var/hoodie/ws

  sparkmaster:
    image: apachehudi/hudi-hadoop_2.8.4-hive_2.3.3-sparkmaster_2.4.4:latest
    hostname: sparkmaster
    env_file:
      - ${HUDI_WS}/docker/compose/hadoop.env
    ports:
      - "$spark_web_port:8080"
      # - "$spark_master_port:7077"
      # JVM debugging port (will be mapped to a random port on host)
      # - "5005"
    environment:
      - INIT_DAEMON_STEP=setup_spark
    links:
      - "hivemetastore"
      - "hiveserver"
      - "hive-metastore-postgresql"
      - "namenode"

  spark-worker-1:
    image: apachehudi/hudi-hadoop_2.8.4-hive_2.3.3-sparkworker_2.4.4:latest
    hostname: spark-worker-1
    env_file:
      - ${HUDI_WS}/docker/compose/hadoop.env
    depends_on:
      - sparkmaster
    # ports:
      # - "8081:8081"
      # JVM debugging port (will be mapped to a random port on host)
      # - "5005"
    environment:
      - "SPARK_MASTER=spark://sparkmaster:7077"
    links:
      - "hivemetastore"
      - "hiveserver"
      - "hive-metastore-postgresql"
      - "namenode"

  zookeeper:
    image: 'bitnami/zookeeper:3.4.12-r68'
    hostname: zookeeper
    # ports:
      # - "2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    restart: on-failure
    volumes:
      - ${pingcap_demo_path}/.tmp.demo/third_party/zookeeper:/bitnami

  kafka:
    image: 'bitnami/kafka:${KAFKA_VERSION}'
    hostname: kafkabroker
    ports:
      - "$kafka_port:9092"
    environment:
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_LISTENERS=PLAINTEXT://:9092
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${demo_host}:${kafka_port}
    restart: on-failure
    volumes:
      - ${pingcap_demo_path}/.tmp.demo/third_party/kafka:/bitnami

  adhoc-1:
    image: apachehudi/hudi-hadoop_2.8.4-hive_2.3.3-sparkadhoc_2.4.4:latest
    hostname: adhoc-1
    env_file:
      - ${HUDI_WS}/docker/compose/hadoop.env
    depends_on:
      - sparkmaster
    # ports:
      # - '4040:4040'
      # JVM debugging port (mapped to 5006 on the host)
      # - "5006:5005"
    environment:
      - "SPARK_MASTER=spark://sparkmaster:7077"
    links:
      - "hivemetastore"
      - "hiveserver"
      - "hive-metastore-postgresql"
      - "namenode"
    volumes:
      - ${HUDI_WS}:/var/hoodie/ws

  jobmanager:
    image: flink:${FLINK_VERSION}
    volumes:
      - ${env_libs}:/pingcap/env_libs
      - ${pingcap_demo_path}:/pingcap/demo
      - ${pingcap_demo_path}/.tmp.demo/third_party/flink:/tmp
    ports:
      - "$flink_jobmanager_port:8081"
    command: /pingcap/demo/flink-jobmanager.sh
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        jobmanager.memory.process.size: 6048m
    restart: on-failure

  taskmanager:
    image: flink:${FLINK_VERSION}
    volumes:
      - ${env_libs}:/pingcap/env_libs
      - ${pingcap_demo_path}:/pingcap/demo
      - ${pingcap_demo_path}/.tmp.demo/third_party/flink:/tmp
    depends_on:
      - jobmanager
    command: /pingcap/demo/flink-taskmanager.sh
    scale: 1
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 20
        taskmanager.memory.process.size: 6048m
    restart: on-failure

  sql-client:
    image: flink:${FLINK_VERSION}
    # command: /opt/flink/bin/flink-sql-client.sh embedded
    command: bash
    volumes:
      - ${env_libs}:/pingcap/env_libs
      - ${pingcap_demo_path}:/pingcap/demo
      - ${pingcap_demo_path}/.tmp.demo/third_party/flink:/tmp
    depends_on:
      - jobmanager
      - taskmanager
    tty: true
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        rest.address: jobmanager

